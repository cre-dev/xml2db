{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Loading XML into a relational database","text":"<p><code>xml2db</code> is a Python package which allows parsing and loading XML files into a relational database:</p> <ul> <li>it maps automatically a XSD schema with a set of tables in the database</li> <li>it can handle complex XML files which cannot be denormalized into flat tables</li> <li>it works out of the box, without any custom mapping rules.</li> </ul> <p><code>xml2db</code> fits well within an Extract, Load, Transform data pipeline pattern: it  loads XML files into a relational data model which is very close to the source data, yet easy to work with, being flat  database tables.</p>"},{"location":"#how-to-load-xml-files-into-a-database","title":"How to load XML files into a database","text":"<p>Loading XML files into a relational database with <code>xml2db</code> can be as simple as:</p> Loading XML into a database<pre><code>from xml2db import DataModel\n\n# Create a DataModel object from an XSD file\ndata_model = DataModel(\n    xsd_file=\"path/to/file.xsd\", \n    connection_string=\"postgresql+psycopg2://testuser:testuser@localhost:5432/testdb\",\n)\n\n# Parse an XML file based on this XSD schema\ndocument = data_model.parse_xml(xml_file=\"path/to/file.xml\")\n\n# Load data into the database, creating target tables if need be\ndocument.insert_into_target_tables()\n</code></pre> <p>The resulting data model will be very similar with the XSD schema. However, <code>xml2db</code> will perform automatically a few simplifications aimed at limiting the complexity of the resulting data model and the storage footprint. The data model  can be configured, but the above code will work out of the box for most schemas, with reasonable defaults.</p> <p>The raw data loaded into the database can then be processed if need be, using for instance DBT, SQL views or stored procedures aimed at extracting, correcting and formatting the data into more user-friendly tables.</p> <p>This package uses <code>sqlalchemy</code> to interact with the database, so it should work with different database backends.  Automated integration tests run against PostgreSQL, MySQL, MS SQL Server and DuckDB. You may have to install additional  packages to connect to your database (e.g. <code>psycopg2</code> for PostgreSQL, <code>pymysql</code> for MySQL, <code>pyodbc</code> for MS SQL Server or <code>duckdb_engine</code> for DuckDB).</p>"},{"location":"#how-to-visualize-your-data-model","title":"How to visualize your data model","text":"<p><code>xml2db</code> can also generate automatically beautiful visualisations of your data models extracted from an XSD file. It  uses Mermaid to represent database tables and their  relationships.</p> <p>It is useful to visualize your data model in order to further configure it if need be.</p> <p>It looks like this:</p> <pre><code>erDiagram\n    Unavailability_MarketDocument ||--o{ TimeSeries : \"TimeSeries*\"\n    Unavailability_MarketDocument ||--|{ Reason : \"Reason*\"\n    Unavailability_MarketDocument {\n        string mRID\n        string revisionNumber\n        NMTOKEN type\n        NMTOKEN process_processType\n        dateTime createdDateTime\n        string sender_MarketParticipant_mRID\n        NMTOKEN sender_MarketParticipant_marketRole_type\n        string receiver_MarketParticipant_mRID\n        NMTOKEN receiver_MarketParticipant_marketRole_type\n        string unavailability_Time_Period_timeInterval_start\n        string unavailability_Time_Period_timeInterval_end\n        NMTOKEN docStatus_value\n    }\n    TimeSeries ||--o{ Available_Period : \"Available_Period*\"\n    TimeSeries ||--o{ Available_Period : \"WindPowerFeedin_Period*\"\n    TimeSeries ||--o{ Asset_RegisteredResource : \"Asset_RegisteredResource*\"\n    TimeSeries ||--o{ Reason : \"Reason*\"\n    TimeSeries {\n        string mRID\n        NMTOKEN businessType\n        string biddingZone_Domain_mRID\n        string in_Domain_mRID\n        string out_Domain_mRID\n        date start_DateAndOrTime_date\n        time start_DateAndOrTime_time\n        date end_DateAndOrTime_date\n        time end_DateAndOrTime_time\n        NMTOKEN quantity_Measure_Unit_name\n        NMTOKEN curveType\n        string production_RegisteredResource_mRID\n        string production_RegisteredResource_name\n        string production_RegisteredResource_location_name\n        NMTOKEN production_RegisteredResource_pSRType_psrType\n        string production_RegisteredResource_pSRType_powerSystemResources_mRID\n        string production_RegisteredResource_pSRType_powerSystemResources_name\n        float production_RegisteredResource_pSRType_powerSystemResources_nominalP\n    }\n    Available_Period ||--|{ Point : \"Point*\"\n    Available_Period {\n        string timeInterval_start\n        string timeInterval_end\n        duration resolution\n    }\n    Point {\n        integer position\n        decimal quantity\n    }\n    Asset_RegisteredResource {\n        string mRID\n        string name\n        NMTOKEN asset_PSRType_psrType\n        string location_name\n    }\n    Reason {\n        NMTOKEN code\n        string text\n    }</code></pre>"},{"location":"#how-to-contribute-to-this-project","title":"How to contribute to this project","text":"<p><code>xml2db</code> is developed and used at the French energy regulation authority (CRE) to process complex XML data.</p> <p>Contributions are welcome, as well as bug reports, starting on the project's  issue page.</p> <p>If you find this package useful, you can give it a star on <code>xml2db</code>'s GitHub repo!</p>"},{"location":"configuring/","title":"Configuring your data model","text":"<p>The data model in the database is derived automatically from a XML schema definition file (XSD) you provide. It is a set of tables linked by foreign keys relationships. Basically, each <code>complexType</code> of the XML schema definition corresponds  to a table in the target database data model. Each table is named after the first element name of this type, with  de-duplication if needed. Columns in a table corresponds to <code>simpleType</code> elements within a complex type and its  attributes. Columns are named after the names of children XML elements or attributes.</p> <p><code>xml2db</code> applies a few simplifications to the original data model by default, but they can also be opted-out or forced  through the configuration <code>dict</code> provided to the <code>DataModel</code> constructor.</p> <p>The column types can also be configured to override the default type mapping, using <code>sqlalchemy</code> types.</p> <p>Tip</p> <p>We recommend that you first build the data model without any configuration, visualize it as a text tree or ER  diagram (see the Getting started page for directions on how to visualize data models) and  then adapt the configuration if need be.</p> <p>Configuration options are described below. Some options can be set at the model level, others at the table level and others at the field level. The general structure of the configuration dict is the following:</p> Model config general structure<pre><code>{\n    \"document_tree_hook\": None,\n    \"document_tree_node_hook\": None,\n    \"row_numbers\": False,\n    \"as_columnstore\": False,\n    \"metadata_columns\": None,\n    \"tables\": {\n        \"table1\": {\n            \"reuse\": True,\n            \"choice_transform\": False,\n            \"as_columnstore\": False,\n            \"fields\": {\n                \"my_column\": {\n                    \"type\": None #default type\n                } \n            },\n            \"extra_args\": [],\n        }\n    }\n}\n</code></pre>"},{"location":"configuring/#model-configuration","title":"Model configuration","text":"<p>The following options can be passed as a top-level keys of the model configuration <code>dict</code>:</p> <ul> <li><code>document_tree_hook</code> (<code>Callable</code>): sets a hook function which can modify the data extracted from the XML. It gives direct access to the underlying tree data structure just before it is extracted to be loaded to the database. This can be used, for instance, to prune or modify some parts of the document tree before loading it into the database. The document tree should of course stay compatible with the data model.</li> <li><code>document_tree_node_hook</code> (<code>Callable</code>): sets a hook function which can modify the data extracted from the XML. It is similar with <code>document_tree_hook</code>, but it is call as soon as a node is completed, not waiting for the entire parsing to finish. It is especially useful if you intend to filter out some nodes and reduce memory footprint while parsing.</li> <li><code>row_numbers</code> (<code>bool</code>): adds <code>xml2db_row_number</code> columns either to <code>n-n</code> relationships tables, or directly to data tables when  deduplication of rows is opted out. This allows recording the original order of elements in the source XML, which is not always respected otherwise. It was implemented primarily for round-trip tests, but could serve other purposes. The  default value is <code>False</code> (disabled).</li> <li><code>as_columnstore</code> (<code>bool</code>): for MS SQL Server, create clustered columnstore indexes on all tables. This can be also set up at the table level for each table. However, for <code>n-n</code> relationships tables, this option is the only way to configure the clustered columnstore indexes. The default value is <code>False</code> (disabled).</li> <li><code>metadata_columns</code> (<code>list</code>): a list of extra columns that you want to add to the root table of your model. This is useful for instance to add the name of the file which has been parsed, or a timestamp, etc. Columns should be specified as dicts, the only required keys are <code>name</code> and <code>type</code> (a SQLAlchemy type object); other keys will be passed directly as keyword arguments to <code>sqlalchemy.Column</code>. Actual values need to be passed to  <code>DataModel.parse_xml</code> for each  parsed documents, as a <code>dict</code>, using the <code>metadata</code> argument.</li> <li><code>record_hash_column_name</code>: the column name to use to store records hash data (defaults to <code>xml2db_record_hash</code>).</li> <li><code>record_hash_constructor</code>: a function used to build a hash, with a signature similar to <code>hashlib</code> constructor  functions (defaults to <code>hashlib.sha1</code>).</li> <li><code>record_hash_size</code>: the byte size of the record hash (defaults to 20, which is the size of a <code>sha-1</code> hash).</li> </ul>"},{"location":"configuring/#fields-configuration","title":"Fields configuration","text":"<p>These configuration options are defined for a specific field of a specific table. A \"field\" refers to a column in the table, or a child table.</p>"},{"location":"configuring/#data-types","title":"Data types","text":"<p>By default, the data type defined in the database table for each column is based on a mapping between the data type  indicated in the XSD and a corresponding <code>sqlalchemy</code> type implemented in the following three functions:</p> Default: <code>types_mapping_default</code> <p>Defines the sqlalchemy type to use for given column properties in target tables</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>bool</code> <p>are we targeting the temporary tables schema or the final tables?</p> required <code>col</code> <code>DataModelColumn</code> <p>an object representing a column of a table for which we are determining the SQL type to define</p> required <p>Returns:</p> Type Description <code>Any</code> <p>a sqlalchemy class representing the data type to be used</p> Source code in <code>xml2db/table/column.py</code> <pre><code>def types_mapping_default(temp: bool, col: \"DataModelColumn\") -&gt; Any:\n    \"\"\"Defines the sqlalchemy type to use for given column properties in target tables\n\n    Args:\n        temp: are we targeting the temporary tables schema or the final tables?\n        col: an object representing a column of a table for which we are determining the SQL type to define\n\n    Returns:\n        a sqlalchemy class representing the data type to be used\n    \"\"\"\n    if col.occurs[1] != 1:\n        return String(8000)\n    if col.data_type in [\"decimal\", \"float\", \"double\"]:\n        return Double\n    if col.data_type == \"dateTime\":\n        return DateTime(timezone=True)\n    if col.data_type in [\n        \"integer\",\n        \"int\",\n        \"nonPositiveInteger\",\n        \"nonNegativeInteger\",\n        \"positiveInteger\",\n        \"negativeInteger\",\n    ]:\n        return Integer\n    if col.data_type == \"boolean\":\n        return Boolean\n    if col.data_type in [\"short\", \"byte\"]:\n        return SmallInteger\n    if col.data_type == \"long\":\n        return BigInteger\n    if col.data_type == \"date\":\n        return String(16)\n    if col.data_type == \"time\":\n        return String(18)\n    if col.data_type in [\"string\", \"NMTOKEN\", \"duration\", \"token\"]:\n        if col.max_length is None:\n            return String(1000)\n        min_length = 0 if col.min_length is None else col.min_length\n        if min_length &gt;= col.max_length - 1 and not col.allow_empty:\n            return String(col.max_length)\n        return String(col.max_length)\n    if col.data_type == \"binary\":\n        return LargeBinary(col.max_length)\n    else:\n        logger.warning(\n            f\"unknown type '{col.data_type}' for column '{col.name}', defaulting to VARCHAR(1000) \"\n            f\"(this can be overridden by providing a field type in the configuration)\"\n        )\n        return String(1000)\n</code></pre> MySQL: <code>types_mapping_mysql</code> <p>Defines the MySQL/sqlalchemy type to use for given column properties in target tables</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>bool</code> <p>are we targeting the temporary tables schema or the final tables?</p> required <code>col</code> <code>DataModelColumn</code> <p>an object representing a column of a table for which we are determining the SQL type to define</p> required <p>Returns:</p> Type Description <code>Any</code> <p>a sqlalchemy class representing the data type to be used</p> Source code in <code>xml2db/table/column.py</code> <pre><code>def types_mapping_mysql(temp: bool, col: \"DataModelColumn\") -&gt; Any:\n    \"\"\"Defines the MySQL/sqlalchemy type to use for given column properties in target tables\n\n    Args:\n        temp: are we targeting the temporary tables schema or the final tables?\n        col: an object representing a column of a table for which we are determining the SQL type to define\n\n    Returns:\n        a sqlalchemy class representing the data type to be used\n    \"\"\"\n    if col.occurs[1] != 1:\n        return String(4000)\n    if col.data_type in [\"string\", \"NMTOKEN\", \"duration\", \"token\"]:\n        if col.max_length is None:\n            return String(255)\n    if col.data_type == \"binary\":\n        if col.max_length == col.min_length:\n            return mysql.BINARY(col.max_length)\n        return mysql.VARBINARY(col.max_length)\n    return types_mapping_default(temp, col)\n</code></pre> MSSQL: <code>types_mapping_mssql</code> <p>Defines the MSSQL type to use for given column properties in target tables</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>bool</code> <p>are we targeting the temporary tables schema or the final tables?</p> required <code>col</code> <code>DataModelColumn</code> <p>an object representing a column of a table for which we are determining the SQL type to define</p> required <p>Returns:</p> Type Description <code>Any</code> <p>a sqlalchemy class representing the data type to be used</p> Source code in <code>xml2db/table/column.py</code> <pre><code>def types_mapping_mssql(temp: bool, col: \"DataModelColumn\") -&gt; Any:\n    \"\"\"Defines the MSSQL type to use for given column properties in target tables\n\n    Args:\n        temp: are we targeting the temporary tables schema or the final tables?\n        col: an object representing a column of a table for which we are determining the SQL type to define\n\n    Returns:\n        a sqlalchemy class representing the data type to be used\n    \"\"\"\n    if col.occurs[1] != 1:\n        return mssql.VARCHAR(8000)\n    if col.data_type == \"dateTime\":\n        # using the DATETIMEOFFSET directly in the temporary table caused issues when inserting data in the target\n        # table with INSERT INTO SELECT converts datetime VARCHAR to DATETIMEOFFSET without errors\n        return mssql.VARCHAR(100) if temp else mssql.DATETIMEOFFSET\n    if col.data_type == \"date\":\n        return mssql.VARCHAR(16)\n    if col.data_type == \"time\":\n        return mssql.VARCHAR(18)\n    if col.data_type in [\"string\", \"NMTOKEN\", \"duration\", \"token\"]:\n        if col.max_length is None:\n            return mssql.VARCHAR(1000)\n        min_length = 0 if col.min_length is None else col.min_length\n        if min_length &gt;= col.max_length - 1 and not col.allow_empty:\n            return mssql.CHAR(col.max_length)\n        return mssql.VARCHAR(col.max_length)\n    if col.data_type == \"binary\":\n        if col.max_length == col.min_length:\n            return mssql.BINARY(col.max_length)\n        return mssql.VARBINARY(col.max_length)\n    return types_mapping_default(temp, col)\n</code></pre> <p>You may override this mapping by specifying a column type for any field in the model config. Custom column types are  defined as <code>sqlalchemy</code> types and will be passed to the <code>sqlalchemy.Column</code> constructor as is.</p> <p>Example</p> <p>If the XSD mentions the <code>integer</code> type for column <code>my_column</code> in table <code>my_table</code>, by default, <code>xml2db</code>will map  to <code>sqlalchemy.Integer</code>. For instance, if you want it to map to <code>mssql.BIGINT</code> instead, you can provide this config:</p> <pre><code>import xml2db\nfrom sqlalchemy.dialects import mssql\n\nmodel_config = {\n    \"tables\": {\n        \"my_table\": {\n            \"fields\": {\n                \"my_column\": {\n                    \"type\": mssql.BIGINT\n                }\n            }\n        },\n    },\n}\n\ndata_model = xml2db.DataModel(\n    xsd_file=\"path/to/file.xsd\", db_schema=\"my_schema\", model_config=model_config\n)\n</code></pre> <p>You can infer <code>my_table</code> and <code>my_column</code> when visualizing the data model.</p>"},{"location":"configuring/#joining-values-for-simple-types","title":"Joining values for simple types","text":"<p>By default, XML simple type elements with types in <code>[\"string\", \"date\", \"dateTime\", \"NMTOKEN\", \"time\"]</code> and max  occurrences &gt;= 1 are joined in one column as comma separated values and optionally wrapped in double quotes if they  contain commas (an Excel-like csv format, which can be queried with <code>LIKE</code> statements in SQL).</p> <p>Configuration: <code>\"transform\":</code> <code>\"join\"</code> (default). It is not currently possible to use <code>False</code> to opt-out of an automatically applied <code>join</code>, as it would require a complex process of adding a new table.</p> <p>Example</p> <p>This config option is currently not very useful as it cannot be opted out. <pre><code>model_config = {\n   \"tables\": {\n       \"my_table_name\": {\n           \"fields\": {\n               \"my_field_name\": {\n                   \"transform\": \"join\"\n               }\n           }\n       }\n   }\n}\n</code></pre></p>"},{"location":"configuring/#elevate-children-to-upper-level","title":"Elevate children to upper level","text":"<p>If a complex child element has a minimum and maximum occurrences number of 1 and 1 respectively, it can be \"pulled\" up  to its parent element. This behaviour will always be applied by default.</p> <p>If a complex child element has a minimum and maximum occurrences number of 0 and 1 respectively, it can also be \"pulled\" up to its parent element fields. This is applied by default if the child has less than 5 fields, because otherwise it could clutter the parent element with many columns that will often be all <code>NULL</code>.</p> <p>This simplification can be opted out using a configuration option, and forced in the case of a child with more than 5 fields, using the following option:</p> <p><code>\"transform\":</code> <code>\"elevate\"</code> (default) or <code>\"elevate_wo_prefix\"</code> or <code>False</code> (disable).</p> <p>By default, the elevated field name is prefixed with the name of the complex child so its origin is clear and to prevent  duplicated names, but this prefixing can be avoided with the value <code>\"elevate_wo_prefix\"</code>.</p> <p>For example, complex child <code>timeInterval</code> with 2 fields of max occurrence 1, before elevation... <pre><code># Child table\ntimeInterval[1, 1]:\n    start[1, 1]: string\n    end[1, 1]: string\n</code></pre></p> <p>... and after elevation (with prefix): <pre><code># Parent fields\ntimeInterval_start[1, 1]: string\ntimeInterval_end[1, 1]: string\n</code></pre></p> <p>Example</p> <p>Force \"elevation\" of a complex type to its parent: <pre><code>model_config = {\n    \"tables\": {\n        \"contract\": {\n            \"fields\": {\n                \"docStatus\": {\n                    \"transform\": \"elevate\"\n                }\n            }\n        }\n    }\n}\n</code></pre></p>"},{"location":"configuring/#tables-configuration","title":"Tables configuration","text":""},{"location":"configuring/#simplify-choice-groups","title":"Simplify \"choice groups\"","text":"<p>In XML schemas, choice groups are quite frequent. It means that only one of its possible children types should be  present. </p> <p>Here we consider only choice groups of simple elements (not complex types). The naive way to convert this to a table is to create one column for each possible choice, of which only one will have a non <code>NULL</code> value for each record.</p> <p>If there are more than 2 possible choice options and the simple elements are of the same type, they can be transformed  into two columns:</p> <ul> <li><code>type</code> with the name of the element</li> <li><code>value</code> with its value</li> </ul> <p>Example of choice child in a table, before... <pre><code>idOfMarketParticipant[1, 1] (choice):\n   lei[1, 1]: string\n   bic[1, 1]: string\n   eic[1, 1]: string\n   gln[1, 1]: string\n</code></pre></p> <p>... and after choice transformation: <pre><code>idOfMarketParticipant[1, 1] (choice):\n   type[1, 1]: string  # with possible values: [\"lei\", \"bic\", \"eic\", \"gln\"]\n   value[1, 1]: string\n</code></pre></p> <p>This simplification is applied by default when there are more than 2 options of the same data type, but it can be opted in or out otherwise, with the following option: </p> <p><code>\"choice_transform\":</code> <code>True</code> (default) or <code>False</code> (disable)</p> <p>Example</p> <p>Disable choice group simplification for a choice group: <pre><code>model_config = {\n    \"tables\": {\n        \"my_table_name\": {\n            \"choice_transform\": False\n        }\n    }\n}\n</code></pre></p>"},{"location":"configuring/#deduplication","title":"Deduplication","text":"<p>By default, <code>xml2db</code> will try to deduplicate elements (store identical element only once in the database) in order to reduce storage footprint, which is particularly relevant for \"feature\" fields in XML schemas, meaning when a XML element  specify a feature as a child element, which is shared with many other elements.</p> <p>This is done using a hash of each node in the XML file, which includes recursively all its children. The detailed  process is described in the how it works page.</p> <p>The implication is that relationships with 1-1 or 1-n cardinality in the XML schema are converted by default into  n-1 and n-n relationships in the database. For n-n, relationships, it means that there is an additional relationship table which has foreign keys relations to both tables in the relationship.</p> <p>This behaviour can be opted-out, for instance if you know that there will be mostly unique elements and you prefer not having the additional relationship table. The 1-n relationship will be modelled using only a foreign key to the parent,  without an intermediate table holding the relationship, which makes the data model simpler, and maybe some queries  faster, but stores more records in case of duplicated records.</p> <p>Configuration: <code>\"reuse\":</code> <code>True</code> (default) or <code>False</code> (disable)</p> <p>Example</p> <p>Disabling deduplication for a given table: <pre><code>model_config = {\n    \"tables\": {\n        \"my_table\": {\"reuse\": False}\n    }\n}\n</code></pre></p>"},{"location":"configuring/#columnstore-clustered-index","title":"Columnstore Clustered Index","text":"<p>With MS SQL Server database backend, <code>xml2db</code> can create  Clustered Columnstore indexes on tables. However, for <code>n-n</code> relationships tables, this option needs to be set globally (see below). The default value  is <code>False</code> (disabled).</p>"},{"location":"configuring/#extra-arguments","title":"Extra arguments","text":"<p>Extra arguments can be passed to <code>sqlalchemy.Table</code> constructors, for instance if you want to customize indexes. These can be passed in an iterable (e.g. <code>tuple</code> or <code>list</code>) which will be simply unpacked into the <code>sqlalchemy.Table</code>  constructor when building the table.</p> <p>Configuration: <code>\"extra_args\": []</code> (default)</p> <p>Example</p> <p>Adding an index on a specific column: <pre><code>model_config = {\n    \"tables\": {\n        \"my_table\": {\n            \"extra_args\": sqlalchemy.Index(\"my_index\", \"my_column1\", \"my_column2\"),\n        }\n    }\n}\n</code></pre></p>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>The package can be installed, preferably in a virtual environment, using <code>pip</code>:</p> <pre><code>pip install xml2db\n</code></pre> <p>Note</p> <p>If you want to contribute to the development of <code>xml2db</code>, clone the git repository and then install it in your  virtual environment in editable mode. From the project's directory, run:</p> <pre><code>pip install -e .[docs,tests]\n</code></pre>"},{"location":"getting_started/#reading-an-xml-schema","title":"Reading an XML schema","text":"<p>Start from an XSD schema file that you will read with <code>xml2db</code> to create a <code>DataModel</code> object:</p> Create a DataModel object<pre><code>from xml2db import DataModel\n\ndata_model = DataModel(\n    xsd_file=\"path/to/file.xsd\",\n    db_schema=\"source_data\", # the name of the database target schema\n    connection_string=\"postgresql+psycopg2://testuser:testuser@localhost:5432/testdb\",\n    model_config={},\n)\n</code></pre> <p>At this stage, it is not required to provide a connection string and have an actual database set up, but it will be  necessary if you want to use this <code>DataModel</code> to actually import data. You may need to install the Python package of the connector you use in your <code>sqlalchemy</code> connection string (<code>psycopg2</code> in the example above).</p> <p>You can provide an optional model configuration, which will allow forcing or preventing some schema simplification, set columns types manually, etc. By default, some simplifications will be applied when possible, in order to limit the  resulting data model complexity.</p>"},{"location":"getting_started/#visualizing-the-data-model","title":"Visualizing the data model","text":"<p>When you start from a new XML schema, we recommend that you first visualize the resulting data model and decide whether it needs some tweaking. The simplest solution will be to generate a markdown page which contains a visual representation of your schema (<code>data_model</code> being the <code>DataModel</code> object previously created):</p> Write an Entity Relationship Diagram to a file<pre><code>with open(f\"target_data_model_erd.md\", \"w\") as f:\n   f.write(data_model.get_entity_rel_diagram())\n</code></pre> <p>You can see an example of these diagrams on the Introduction page.</p> <p>The data model visualization uses Mermaid to create an \"entity relationship diagram\" which will show the tables created and the relationships between them. To visualize this, you can for instance rely on Pycharm IDE mermaid support. GitHub will also natively support those.</p> <p>You can also visualize your model in a tree-like text mode. In this format, you can visualize the raw, untouched XML schema, as well as the simplified one (we call it \"target\" model):</p> Write source tree and target tree to a file<pre><code>with open(f\"source_tree.txt\", \"w\") as f:\n    f.write(data_model.source_tree)\n\nwith open(f\"target_tree.txt\", \"w\") as f:\n    f.write(data_model.target_tree)\n</code></pre> <p>It will write something like this:</p> <pre><code>...\ndocStatus_value[0, 1]: NMTOKEN\nTimeSeries[0, None]:\n    mRID[1, 1]: string\n    businessType[1, 1]: NMTOKEN\n    quantity_Measure_Unit.name[1, 1]: NMTOKEN\n    curveType[1, 1]: NMTOKEN\n    Available_Period[0, None]:\n        timeInterval_start[1, 1]: string\n        timeInterval_end[1, 1]: string\n        resolution[1, 1]: duration\n        Point[1, None]:\n            position[1, 1]: integer\n            quantity[1, 1]: decimal\n    WindPowerFeedin_Period[0, None]:\n        timeInterval_start[1, 1]: string\n        timeInterval_end[1, 1]: string\n...\n</code></pre> <p>This gives you the elements names, data type and cardinality (min/max number of children elements). </p> <p>It is useful to visualize your data model in order to configure it to suit your needs.</p>"},{"location":"getting_started/#importing-xml-files","title":"Importing XML files","text":"<p>Once you are happy with the data model created from previous steps, you are now ready to actually process XML files and load their content to your database. It goes like this:</p> Parse a XML file<pre><code>document = data_model.parse_xml(\n    xml_file=\"path/to/file.xml\",\n)\ndocument.insert_into_target_tables()\n</code></pre> <p>By default, the validity of your XML file will not be checked against the XML schema used to create your <code>data_model</code>  object, which can be enabled if you are unsure that your XML files will be valid.</p> <p>The <code>Document.insert_into_target_tables</code> method is then all you need to load your data to the database.</p> <p>Please read the How it works page to learn more about the process, which could help  troubleshooting if need be.</p> <p>Note</p> <p><code>xml2db</code> can save metadata for each loaded XML file. These can be configured using the  <code>metadata_columns</code> option and create additional columns in the root table. It can be used for instance to save file name or loading timestamp.</p> <p>Actual values need to be passed to <code>DataModel.parse_xml</code> for  each parsed documents, as a <code>dict</code>, using the <code>metadata</code> argument.</p> <p>Note</p> <p>You can also load multiple documents at the same time to the database, which could make the process faster if you  have a lot of small XML files to load: <pre><code>data = None\nfor xml_file in files:\n    document = data_model.parse_xml(\n        xml_file=\"path/to/file.xml\",\n        flat_data=data,\n    )\n    data = document.data\ndocument.insert_into_target_tables()\n</code></pre></p>"},{"location":"getting_started/#getting-back-the-data-into-xml","title":"Getting back the data into XML","text":"<p>You can extract the data from the database into XML files. This was implemented primarily to be able to test the package using \"round trip\" tests to and from the database.</p> Extract data back to XML<pre><code>document = data_model.extract_from_database(\n    root_select_where=\"xml2db_input_file_path='path/to/file.xml'\",\n)\ndocument.to_xml(\"extracted_file.xml\")\n</code></pre>"},{"location":"how_it_works/","title":"How it works","text":"<p>This page covers more advanced topics to understand in depth how data models are created and how data is loaded to the database. This can help with troubleshooting or for advanced use cases.</p>"},{"location":"how_it_works/#building-a-data-model","title":"Building a data model","text":"<p>A XML document is a tree-like structure where every element is either a simple type (i.e. a scalar value) or a complex type which has children which can be simple types or complex types. Elements can also have attributes, which are also scalar values, which <code>xml2db</code> will handle just as simple type children. In this page we will call \"properties\" the  simple type children of an element or its attributes.</p> <p>The general idea of <code>xml2db</code> is to convert complex types to a database tables, and simple types or XML attributes into  columns in these tables. When a complex type has complex type children, they are themselves stored in other tables, and  related to their parents with a foreign key constraint, or a relationship table which holds foreign key constraints to  both related tables.</p> <p>The data model created by <code>xml2db</code> is mostly bijective with the original XML document, which can in many cases be  extracted and converted back to a XML document as it was loaded into the database.</p> <p>Note</p> <p>XSD specification allows multiple-root schemas. This means that XML documents conforming to this schema can have different root elements (XML files will always have just one root element, but they can be different between  different XML files conforming to the same schema).</p> <p><code>xml2db</code> handles this by creating a \"virtual\" root table, named after the <code>short_name</code> provided to the <code>DataModel</code> constructor, which will act as a root node in all XML files loaded in this data model.</p>"},{"location":"how_it_works/#hash-based-deduplication","title":"Hash-based deduplication","text":"<p>By default, <code>xml2db</code> tries to reduce storage footprint in the database by storing only once every subtree from the  original XML document, if it is already present in the database. This deduplication process takes into account the whole subtree starting from a given element, and not only the direct children of an element.</p> <p>Taking advantage of the initial tree structure, after parsing the XML document into a python dict, we compute a hash for each node, which includes all its properties and all its children hash, recursively. Two nodes with the same hash are  thus identical, so only one of them needs to be stored, even if they appear under different parent nodes.</p> <p>Hash are stored in the database, with a unique constraint (as a column with binary type named <code>xml2db_record_hash</code>). The primary key of all databases is an auto-incremented integer column, always named <code>pk_table_name</code>, <code>table_name</code> being the name of the table.</p> <p>In some cases, especially when only few  duplicates are expected, it may be more efficient to allow duplicated nodes in  order to avoid extra tables to store relationships. This can be configured for each table of the data model.</p>"},{"location":"how_it_works/#modeling-relationships","title":"Modeling relationships","text":"<p>Within a tree, parent-child relationship can have <code>1-1</code> or <code>1-n</code> cardinality. As we want to reuse child nodes, we convert these relationships to <code>n-1</code> or <code>n-n</code>, respectively. Besides, some children are optional. This does not affect the representation of relationships in any way.</p> <p>A same child node (same hash) which is used under different parents will have several parents after the \"recycling\" process, while it had only one parent (because of the tree structure) in the initial dataset. This example illustrates a <code>1-1</code> relationship converted to <code>n-1</code> after reusing nodes:</p> <pre><code>erDiagram\n          TRADE ||--|| CONTRACT : concerns\n          UNIQUE_TRADE }|--|| UNIQUE_CONTRACT : concerns</code></pre> <p>In that case, <code>UNIQUE_TRADE</code> holds a foreign key relationship with <code>UNIQUE_CONTRACT</code>, i.e. it has a column named  <code>fk_UNIQUE_CONTRACT</code> which contains the primary key values of related <code>UNIQUE_CONTRACT</code> table (named  <code>pk_UNIQUE_CONTRACT</code> in the <code>UNIQUE_CONTRACT</code> table).</p> <p>Note</p> <p>The form of each end of the links in the entity relationships diagram represent the cardinality of the relationship, which can be exactly 1, 0 or 1, 0 or many, 1 or many. You can find the symbols here. </p> <p>For <code>1-n</code> relationships, the same idea applies: a child node can have multiple parents if it was used under different parent nodes.</p> <pre><code>erDiagram\n          CONTRACT ||--|{ DELIVERY_PROFILE : delivers\n          UNIQUE_CONTRACT }|--|{ UNIQUE_DELIVERY_PROFILE : delivers</code></pre> <p>In a SQL relational data model, <code>1-n</code> relationships can be easily represented with foreign keys. <code>n-n</code> relationships  require  however an additional table holding the relationship, which gives, for the last example with contracts and  delivery profiles:</p> <pre><code>erDiagram\n          CONTRACT ||--|{ CONTRACT_DELIVERY_PROFILE : is_in\n          CONTRACT_DELIVERY_PROFILE }|--|| DELIVERY_PROFILE : involves</code></pre> <p>Note</p> <p>By default, these <code>n-n</code> relationships which involve a third table to hold the relationship are not explicitly shown in the data model visualisation generated by  <code>DataModel.get_entity_rel_diagram</code>, to favor  readability. However, the relationships links on the diagram show an asterisk (*) when a relationship table is  involved. These relationship tables are named using the name of the two related tables separated with an underscore.</p>"},{"location":"how_it_works/#duplicated-elements","title":"Duplicated elements","text":"<p>As explained previously, deduplication can be opted out to avoid the complexity of an intermediary table holding a <code>n-n</code> relationship. In that case, the relationship will stay a <code>1-n</code> relationship, which will be modeled with the child  element holding a foreign key relationship to its parent.</p> <p>In that case, no hash is stored for the children as there are effectively duplicated elements. The column  <code>fk_parent_tablename</code> in the child table holds the primary keys of associated parent rows in the parent table  <code>tablename</code>.</p> <p>This choice is made for each table individually, and children of a duplicated element can effectively be themselves  stored as deduplicated elements.</p> <p>This means that as the end, there can be both <code>1-n</code> and <code>n-1</code> relationships involved to represent a tree structure, which means that the order of dependencies of the resulting tables will not be the same as the original tree structure. For instance, when we want to make sure to process all dependent tables before processing a table, we won't likely start with the root table of the tree, as it would be expected without the de-duplication process.</p>"},{"location":"how_it_works/#caveats","title":"Caveats","text":"<p><code>xml2db</code> handles a variety of data models, but does not cover all possible schemas allowed by the XML schema documents specification.</p> <p>Some known cases which are not supported by <code>xml2db</code> are described below. Other cases can fail and may require some adjustments to work. We recommend thorough testing for more \"exotic\" schemas; for instance it is possible to implement \"round-trip\" tests from sample XML files to database and back to XML, and compare the resulting XML file with the  original one.</p>"},{"location":"how_it_works/#recursive-xsd","title":"Recursive XSD","text":"<p>Recursive XML schemas are not fully supported, because they result in cycles in tables dependencies, which would make the process much more complex. Whenever a field which would introduce a dependency cycle is detected in the XSD, it is  discarded with a warning, which means that the corresponding data in XML files will not be imported. The rest of the data should be processed correctly.</p>"},{"location":"how_it_works/#mixed-content-elements","title":"Mixed content elements","text":"<p>XML elements with mixed content can contain both text and children elements (tags). <code>xml2db</code> offers partial support for this: the text value will be stored in a specific column named <code>value</code>, but it will not record the specific sequence of text and children elements within a node. When using  <code>Document.to_xml</code>, text value will always appear before children  elements.</p>"},{"location":"how_it_works/#loading-process","title":"Loading process","text":"<p>This section gives more detailed explanations on how parsing and loading data work. The integration of an XML file can  be decomposed in lower level steps described below.</p>"},{"location":"how_it_works/#parsing-a-xml-document","title":"Parsing a XML document","text":"<p>First, we load all the XML document in memory using <code>lxml</code> and extract the data as a nested <code>dict</code>, where each node keeps a reference of its type, and store its data content. This task is achieved by the function  <code>XMLConverter.parse_xml</code>.</p> <p>This limits the size of files that can be loaded, due to memory limitations. The merging database transaction also  limits the size of the files that can be loaded, depending on the server performance. On the other hand, handling data in memory makes the processing way simpler and faster. We handle files with a size around 500 MB without any issue.</p>"},{"location":"how_it_works/#computing-hashes","title":"Computing hashes","text":"<p>We compute tree hashes  recursively by adding to each node's hash the hashes of its children element, be it simple  types, attributes or complex types. Children are processed in the specific order they appeared in the XSD schema,  so that hashing is really deterministic.</p> <p>Right after this step, a hook function is called if provided in the configuration (top-level <code>document_tree_hook</code> option in the configuration <code>dict</code>), which gives direct access to the underlying tree data structure just before it is  extracted to be loaded to the database. This can be used, for instance, to prune or modify some parts of the document  tree.</p>"},{"location":"how_it_works/#extracting-data","title":"Extracting data","text":"<p>We extract data from the tree structure based on the type (table name) they belong to, walking the data tree down. Doing so, we create temporary incremental primary keys for each row and perform deduplication when it is needed, using  the hashes previously computed. We store relationship as they will be stored in the database, using the temporary  primary key of the other table related rows in a foreign key column.</p> <p>At this stage, we have a data representation that matches the one we will find in the database, except that it contains  only the data from the XML file we just parsed, and the final primary keys and foreign keys won't be the same.</p>"},{"location":"how_it_works/#loading-the-data","title":"Loading the data","text":"<p>The data we converted is then loaded to the database in a separate set of tables, which have the same names that the  target tables, but prefixed with <code>temp_XXX</code> (with <code>XXX</code> being a random 8 characters <code>uuid</code> string, by default).</p> <p>We keep the primary keys from the flat data model created at the previous stage, as temporary keys.</p> <p>Tip</p> <p>The <code>temp_prefix</code> argument can be passed to the constructor of <code>DataModel</code> to use a specific prefix instead of the  random one, which can be useful if you want to decompose the process of loading data and merging it with the target tables later, for instance to gain a finer control over concurrency.</p>"},{"location":"how_it_works/#merging-the-data","title":"Merging the data","text":"<p>The last step is to merge the temporary tables data into the target tables, while enforcing deduplication, keeping  relationships, etc.</p> <p>This is done by issuing a sequence of <code>update</code> and <code>insert</code> SQL statements using <code>sqlalchemy</code>, in a single transaction (default) or in multiple transactions.</p> <p>The process boils down to:</p> <ul> <li>inserting missing records into the target tables, </li> <li>getting back the auto-incremented primary keys into the temporary tables,</li> <li>updating relationship to use target primary keys instead of temporary primary keys,</li> <li>continue with the next table.</li> </ul>"},{"location":"how_it_works/#summing-up","title":"Summing up","text":"<p>The whole loading process can be achieved by the high level functions  <code>DataModel.parse_xml</code> and  <code>Document.insert_into_target_tables</code>. However,  the later can be decomposed in lower level function calls, for instance if you want to separate the loading to the  temporary tables and the merge operation into the target tables. You can have a look at  <code>Document.insert_into_target_tables</code>  source code to see how the lower level steps are stitched together.</p>"},{"location":"how_it_works/#extracting-the-data-back-to-xml","title":"Extracting the data back to XML","text":"<p>Extracting the data from the database and converting it back to XML follow similar steps, in reverse order.</p> <p>Info</p> <p>Extracting the data from the database is not very optimized and is actually currently quite slow, mostly due to complex join queries to retrieve data based on a filter only on the top node. This feature is currently useful for roundtrip test, but has limited value otherwise, because of its poor performance compared to loading.</p>"},{"location":"how_it_works/#querying-data-from-the-database","title":"Querying data from the database","text":"<p>Walking through the data model tree, we query all tables using a chain of joins to the root table, on which we apply the where clause provided to the function  <code>DataModel.extract_from_database</code>. Results are stored  in a <code>Document</code> instance in flat tables, with their primary keys and foreign keys taken from the database.</p>"},{"location":"how_it_works/#converting-flat-data-to-document-tree","title":"Converting flat data to document tree","text":"<p>Starting from the flat data representation, we build a document tree recursively. This is done by the function  <code>Document.flat_data_to_doc_tree</code>, which performs the  opposite conversion as <code>Document.doc_tree_to_flat_data</code>.</p>"},{"location":"how_it_works/#building-an-xml-file","title":"Building an XML file","text":"<p>From the document tree, we build an XML file using  <code>XMLConverter.to_xml</code>. This conversion is reversible  with some caveat, regarding mostly the formatting of numbers and dates.</p>"},{"location":"api/data_model/","title":"DataModel","text":"<p>A class to manage a data model based on an XML schema and its database equivalent.</p> <p>It is the main entry point for <code>xml2db</code>.</p> <p>This class allows parsing an XSD file to build  a representation of the XML schema, simplify it and convert it into a set of database tables. It also allows parsing XML documents that fit this XML schema and importing their content into a database.</p> <p>Parameters:</p> Name Type Description Default <code>xsd_file</code> <code>str</code> <p>A path to a XSD file</p> required <code>short_name</code> <code>str</code> <p>A short name for the schema</p> <code>'DocumentRoot'</code> <code>long_name</code> <code>str</code> <p>A longer name for the schema</p> <code>None</code> <code>base_url</code> <code>str</code> <p>The root folder to find other dependant XSD files (by default, the location of the provided XSD file)</p> <code>None</code> <code>model_config</code> <code>dict</code> <p>A config dict to provide options for building the model (full options available here: Configuring your data model)</p> <code>None</code> <code>connection_string</code> <code>str</code> <p>A database connection string (optional if you will not be loading data)</p> <code>None</code> <code>db_engine</code> <code>Engine</code> <p>A <code>sqlalchemy.Engine</code> to use to connect to the database (it takes precedence over <code>connection_string</code> and is optional if you will not be loading data)</p> <code>None</code> <code>db_type</code> <code>str</code> <p>The targeted database backend (<code>postgresql</code>, <code>mssql</code>, <code>mysql</code>...). It is ignored and inferred from <code>connection_string</code> or <code>db_engine</code>, if provided</p> <code>None</code> <code>db_schema</code> <code>str</code> <p>A schema name to use in the database</p> <code>None</code> <code>temp_prefix</code> <code>str</code> <p>A prefix to use for temporary tables (if <code>None</code>, will be generated randomly)</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>xml_schema</code> <p>The <code>xmlschema.XMLSchema</code> object associated with this data model</p> <code>lxml_schema</code> <p>The <code>lxml.etree.XMLSchema</code> object associated with this data model</p> <code>data_flow_name</code> <p>A short identifier used for the data model (<code>short_name</code> argument value)</p> <code>data_flow_long_name</code> <p>A longer for the data model (<code>long_name</code> argument value)</p> <code>db_schema</code> <p>A database schema name to store the database tables</p> <code>source_tree</code> <p>A text representation of the source data model tree</p> <code>target_tree</code> <p>A text representation of the simplified data model tree which will be used to create target tables</p> <p>Examples:</p> <p>Create a <code>DataModel</code> like this:</p> <pre><code>&gt;&gt;&gt; data_model = DataModel(\n&gt;&gt;&gt;     xsd_file=\"path/to/file.xsd\",\n&gt;&gt;&gt;     connection_string=\"postgresql+psycopg2://testuser:testuser@localhost:5432/testdb\",\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>xml2db/model.py</code> <pre><code>def __init__(\n    self,\n    xsd_file: str,\n    short_name: str = \"DocumentRoot\",\n    long_name: str = None,\n    base_url: str = None,\n    model_config: dict = None,\n    connection_string: str = None,\n    db_engine: sqlalchemy.Engine = None,\n    db_type: str = None,\n    db_schema: str = None,\n    temp_prefix: str = None,\n):\n    self.model_config = self._validate_config(model_config)\n    self.tables_config = model_config.get(\"tables\", {}) if model_config else {}\n\n    xsd_file_name = xsd_file\n    if base_url is None:\n        base_url = os.path.normpath(os.path.dirname(xsd_file))\n        xsd_file_name = os.path.basename(xsd_file)\n\n    self.xml_schema = xmlschema.XMLSchema(xsd_file_name, base_url=base_url)\n    self.lxml_schema = etree.XMLSchema(etree.parse(xsd_file))\n\n    self.xml_converter = XMLConverter(data_model=self)\n    self.data_flow_name = short_name\n    self.data_flow_long_name = long_name\n\n    if connection_string is None and db_engine is None:\n        logger.warning(\n            \"DataModel created without connection string cannot do actual imports\"\n        )\n        self.engine = None\n        self.db_type = db_type\n    else:\n        if db_engine:\n            self.engine = db_engine\n        else:\n            engine_options = {}\n            if \"mssql\" in connection_string:\n                engine_options = {\n                    \"fast_executemany\": True,\n                    \"isolation_level\": \"SERIALIZABLE\",\n                }\n            self.engine = create_engine(\n                connection_string,\n                **engine_options,\n            )\n        self.db_type = self.engine.dialect.name\n\n    self.db_schema = db_schema\n    self.temp_prefix = str(uuid4())[:8] if temp_prefix is None else temp_prefix\n\n    self.tables = {}\n    self.names_types_map = {}\n    self.root_table = None\n\n    self.types_transforms = {}\n    self.fields_transforms = {}\n    self.ordered_tables_keys = []\n    self.transaction_groups = []\n    self.source_tree = \"\"\n    self.target_tree = \"\"\n    self.metadata = MetaData()\n    self.processed_at = datetime.now()\n\n    self._build_model()\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.fk_ordered_tables","title":"<code>fk_ordered_tables</code>  <code>property</code>","text":"<p>Yields tables in create/insert order (tables referenced in foreign keys first)</p>"},{"location":"api/data_model/#xml2db.model.DataModel.fk_ordered_tables_reversed","title":"<code>fk_ordered_tables_reversed</code>  <code>property</code>","text":"<p>Yields tables in drop/delete order (tables referencing foreign keys first)</p>"},{"location":"api/data_model/#xml2db.model.DataModel.create_all_tables","title":"<code>create_all_tables(temp=False)</code>","text":"<p>Create tables for the data model, either target tables or temp tables used to import data.</p> <p>You do not have to call this method explicitly when using     <code>Document.insert_into_target_tables()</code>,     which will create tables if they do not exist.</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>bool</code> <p>If <code>False</code>, create target tables (unprefixed). If <code>True</code>, create temporary (prefixed) tables.</p> <code>False</code> Source code in <code>xml2db/model.py</code> <pre><code>def create_all_tables(self, temp: bool = False) -&gt; None:\n    \"\"\"Create tables for the data model, either target tables or temp tables used to import data.\n\n    You do not have to call this method explicitly when using\n        [`Document.insert_into_target_tables()`](document.md#xml2db.document.Document.insert_into_target_tables),\n        which will create tables if they do not exist.\n\n    Args:\n        temp: If `False`, create target tables (unprefixed). If `True`, create temporary (prefixed) tables.\n    \"\"\"\n    for tb in self.fk_ordered_tables:\n        tb.create_tables(self.engine, temp)\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.create_db_schema","title":"<code>create_db_schema()</code>","text":"<p>Create database schema if it does not already exist.</p> <p>You do not have to call this method explicitly when using     <code>Document.insert_into_target_tables()</code>.</p> Source code in <code>xml2db/model.py</code> <pre><code>def create_db_schema(self) -&gt; None:\n    \"\"\"Create database schema if it does not already exist.\n\n    You do not have to call this method explicitly when using\n        [`Document.insert_into_target_tables()`](document.md#xml2db.document.Document.insert_into_target_tables).\n    \"\"\"\n\n    def do_create_schema():\n        with self.engine.connect() as conn:\n            conn.execute(sqlalchemy.schema.CreateSchema(self.db_schema))\n            conn.commit()\n\n    if self.db_schema is not None:\n        if self.db_type == \"duckdb\":\n            try:\n                do_create_schema()\n            except ProgrammingError:\n                pass\n        else:\n            inspector = inspect(self.engine)\n            if self.db_schema not in inspector.get_schema_names():\n                do_create_schema()\n\n        logger.info(f\"Created schema: {self.db_schema}\")\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.drop_all_tables","title":"<code>drop_all_tables()</code>","text":"<p>Drop the data model target (unprefixed) tables.</p> Danger <p>BE CAUTIOUS, THIS METHOD DROPS TABLES WITHOUT FURTHER NOTICE!</p> Source code in <code>xml2db/model.py</code> <pre><code>def drop_all_tables(self):\n    \"\"\"Drop the data model target (unprefixed) tables.\n\n    Danger:\n        BE CAUTIOUS, THIS METHOD DROPS TABLES WITHOUT FURTHER NOTICE!\n    \"\"\"\n    for tb in self.fk_ordered_tables_reversed:\n        tb.drop_tables(self.engine)\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.drop_all_temp_tables","title":"<code>drop_all_temp_tables()</code>","text":"<p>Drop the data model temporary (prefixed) tables.</p> Danger <p>BE CAUTIOUS, THIS METHOD DROPS TABLES WITHOUT FURTHER NOTICE!</p> Source code in <code>xml2db/model.py</code> <pre><code>def drop_all_temp_tables(self):\n    \"\"\"Drop the data model temporary (prefixed) tables.\n\n    Danger:\n        BE CAUTIOUS, THIS METHOD DROPS TABLES WITHOUT FURTHER NOTICE!\n    \"\"\"\n    for tb in self.fk_ordered_tables_reversed:\n        tb.drop_temp_tables(self.engine)\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.extract_from_database","title":"<code>extract_from_database(root_select_where, force_tz=None)</code>","text":"<p>Extract a document from the database, based on a where clause applied to the root table. For instance, you     can use the column <code>xml2db_input_file_path</code> to filter the data loaded from a specific file.</p> <p>It will query all the data in the database corresponding to the rows that you select from the root table of your     data model. Typically, a single XML file will correspond to a single row in the root table. This function     will query the data tree below this record.</p> <p>This method was not optimized for performance and can be quite slow. It is used in integration tests to check     the output against the data inserted into the database.</p> <p>Parameters:</p> Name Type Description Default <code>root_select_where</code> <code>str</code> <p>A where clause to filter the root table of the model, as a string</p> required <code>force_tz</code> <code>Union[str, None]</code> <p>Apply this timezone if database returns timezone-na\u00efve datetime</p> <code>None</code> <p>Returns:</p> Type Description <code>Document</code> <p>A <code>Document</code> object containing extracted data</p> <p>Examples:</p> Source code in <code>xml2db/model.py</code> <pre><code>def extract_from_database(\n    self,\n    root_select_where: str,\n    force_tz: Union[str, None] = None,\n) -&gt; Document:\n    \"\"\"Extract a document from the database, based on a where clause applied to the root table. For instance, you\n        can use the column `xml2db_input_file_path` to filter the data loaded from a specific file.\n\n    It will query all the data in the database corresponding to the rows that you select from the root table of your\n        data model. Typically, a single XML file will correspond to a single row in the root table. This function\n        will query the data tree below this record.\n\n    This method was not optimized for performance and can be quite slow. It is used in integration tests to check\n        the output against the data inserted into the database.\n\n    Args:\n        root_select_where: A where clause to filter the root table of the model, as a string\n        force_tz: Apply this timezone if database returns timezone-na\u00efve datetime\n\n    Returns:\n        A [`Document`](document.md) object containing extracted data\n\n    Examples:\n\n    \"\"\"\n    doc = Document(self)\n    doc.extract_from_database(self.root_table, root_select_where, force_tz=force_tz)\n    return doc\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.get_all_create_index_statements","title":"<code>get_all_create_index_statements()</code>","text":"<p>Yield create index statements for all tables</p> Source code in <code>xml2db/model.py</code> <pre><code>def get_all_create_index_statements(self) -&gt; Iterable[CreateIndex]:\n    \"\"\"Yield create index statements for all tables\"\"\"\n    for tb in self.fk_ordered_tables:\n        yield from tb.get_create_index_statements()\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.get_all_create_table_statements","title":"<code>get_all_create_table_statements(temp=False)</code>","text":"<p>Yield sqlalchemy <code>create table</code> statements for all tables</p> <p>Parameters:</p> Name Type Description Default <code>temp</code> <code>bool</code> <p>If <code>False</code>, yield create table statements for target tables (unprefixed). If True, yield create table statements for temporary tables (prefixed).</p> <code>False</code> Source code in <code>xml2db/model.py</code> <pre><code>def get_all_create_table_statements(\n    self, temp: bool = False\n) -&gt; Iterable[CreateTable]:\n    \"\"\"Yield sqlalchemy `create table` statements for all tables\n\n    Args:\n        temp: If `False`, yield create table statements for target tables (unprefixed). If True, yield create\n            table statements for temporary tables (prefixed).\n    \"\"\"\n    for tb in self.fk_ordered_tables:\n        yield from tb.get_create_table_statements(temp)\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.get_entity_rel_diagram","title":"<code>get_entity_rel_diagram(text_context=True)</code>","text":"<p>Build an entity relationship diagram for the data model</p> <p>The ERD syntax is used by mermaid.js to create a visual representation of the diagram, which is supported by Pycharm IDE or GitHub in markdown files, among others</p> <p>Parameters:</p> Name Type Description Default <code>text_context</code> <code>bool</code> <p>Should we add a title, a text explanation, etc. or just the ERD?</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the ERD</p> Source code in <code>xml2db/model.py</code> <pre><code>def get_entity_rel_diagram(self, text_context: bool = True) -&gt; str:\n    \"\"\"Build an entity relationship diagram for the data model\n\n    The ERD syntax is used by mermaid.js to create a visual representation of the diagram, which is supported\n    by Pycharm IDE or GitHub in markdown files, among others\n\n    Args:\n        text_context: Should we add a title, a text explanation, etc. or just the ERD?\n\n    Returns:\n        A string representation of the ERD\n    \"\"\"\n    out = [\"erDiagram\"]\n    for tb in self.fk_ordered_tables_reversed:\n        out += tb.get_entity_rel_diagram()\n\n    if text_context:\n        out = (\n            [\n                f\"# {self.data_flow_long_name}\\n\",\n                f\"### Data model name: `{self.data_flow_name}`\\n\",\n                (\n                    \"The following *Entity Relationships Diagram* represents the target data model, after the \"\n                    \"simplification of the source data model, but before the transformations performed to optimize \"\n                    \"data storage (transformation of `1-1` and `1-n` relationships into `n-1` and `n-n` \"\n                    \"relationships, respectively.\\n\"\n                ),\n                (\n                    \"As a consequence, not all tables of the actual data model used in the database are shown. \"\n                    \"Specifically, `1-n` relationships presented may be stored in the database using an additional \"\n                    \"relationship table (noted with an asterisk in the relationship name).\\n\"\n                ),\n                \"```mermaid\",\n            ]\n            + out\n            + [\n                \"```\",\n                (\n                    \"`-N` suffix in field type indicates that the field can have multiple values, which will be \"\n                    \"stored as comma separated values.\"\n                ),\n            ]\n        )\n    return \"\\n\".join(out)\n</code></pre>"},{"location":"api/data_model/#xml2db.model.DataModel.parse_xml","title":"<code>parse_xml(xml_file, metadata=None, skip_validation=True, iterparse=True, recover=False, flat_data=None)</code>","text":"<p>Parse an XML document based on this data model</p> <p>This method is just a wrapper around the parse_xml method of the Document class.</p> <p>Parameters:</p> Name Type Description Default <code>xml_file</code> <code>Union[str, BytesIO]</code> <p>The path or the file object of an XML file to parse</p> required <code>metadata</code> <code>dict</code> <p>A dict of metadata values to add to the root table (a value for each key defined in <code>metadata_columns</code> passed to model config)</p> <code>None</code> <code>skip_validation</code> <code>bool</code> <p>Should we validate the documents against the schema first?</p> <code>True</code> <code>iterparse</code> <code>bool</code> <p>Parse XML using iterative parsing, which is a bit slower but uses less memory</p> <code>True</code> <code>recover</code> <code>bool</code> <p>Should we try to parse incorrect XML? (argument passed to lxml parser)</p> <code>False</code> <code>flat_data</code> <code>dict</code> <p>A dict containing flat data if we want to add data to another dataset instead of creating a new one</p> <code>None</code> <p>Returns:</p> Type Description <code>Document</code> <p>A parsed <code>Document</code> object</p> Source code in <code>xml2db/model.py</code> <pre><code>def parse_xml(\n    self,\n    xml_file: Union[str, BytesIO],\n    metadata: dict = None,\n    skip_validation: bool = True,\n    iterparse: bool = True,\n    recover: bool = False,\n    flat_data: dict = None,\n) -&gt; Document:\n    \"\"\"Parse an XML document based on this data model\n\n    This method is just a wrapper around the parse_xml method of the Document class.\n\n    Args:\n        xml_file: The path or the file object of an XML file to parse\n        metadata: A dict of metadata values to add to the root table (a value for each key defined in\n            `metadata_columns` passed to model config)\n        skip_validation: Should we validate the documents against the schema first?\n        iterparse: Parse XML using iterative parsing, which is a bit slower but uses less memory\n        recover: Should we try to parse incorrect XML? (argument passed to lxml parser)\n        flat_data: A dict containing flat data if we want to add data to another dataset instead of creating\n            a new one\n\n    Returns:\n        A parsed [`Document`](document.md) object\n    \"\"\"\n    doc = Document(self)\n    doc.parse_xml(\n        xml_file=xml_file,\n        metadata=metadata,\n        skip_validation=skip_validation,\n        iterparse=iterparse,\n        recover=recover,\n        flat_data=flat_data,\n    )\n    return doc\n</code></pre>"},{"location":"api/document/","title":"Document","text":"<p>A class to represent a single XML file with its data, based on a given XSD.</p> <p>Based on a given DataModel object which represents the data model defined in the XSD, this class deals with the data itself. It allows parsing an XML file to extract the data into the data model format (performing the transforms defined in the DataModel object) and inserting the data into the database.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DataModel</code> <p>A <code>DataModel</code> object for this document</p> required Source code in <code>xml2db/document.py</code> <pre><code>def __init__(self, model: \"DataModel\"):\n    self.model = model\n    self.data = {}\n    self.xml_file_path = None\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.__repr__","title":"<code>__repr__()</code>","text":"<p>Output a repr string for the current document with records count for each table</p> Source code in <code>xml2db/document.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Output a repr string for the current document with records count for each table\"\"\"\n    settings = (\n        f\"temp_prefix: {self.model.temp_prefix}, db_schema: {self.model.db_schema}\"\n    )\n    if not self.data:\n        return f\"Empty {self.model.data_flow_name} document ({settings})\"\n    else:\n        n = sum([len(v[\"records\"]) for v in self.data.values()])\n        return \"\\n\".join(\n            [\n                f\"Parsed {self.xml_file_path} into a {self.model.data_flow_name} document: {n} records\",\n                f\"({settings})\",\n            ]\n            + [\n                f\"   {self.model.tables[k].name}: {len(v['records'])}\"\n                for k, v in self.data.items()\n            ]\n        )\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.doc_tree_to_flat_data","title":"<code>doc_tree_to_flat_data(document_tree, metadata=None, flat_data=None)</code>","text":"<p>Convert document tree (nested dict) to flat tables data model to prepare database import</p> <p>Parameters:</p> Name Type Description Default <code>document_tree</code> <code>tuple</code> <p>A tuple (node_type, content, hash) containing the document tree</p> required <code>metadata</code> <code>dict</code> <p>A dict of metadata values to add to the root table (a value for each key defined in <code>metadata_columns</code> passed to model config)</p> <code>None</code> <code>flat_data</code> <code>dict</code> <p>A dict to store the flat data into</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing flat tables</p> Source code in <code>xml2db/document.py</code> <pre><code>def doc_tree_to_flat_data(\n    self, document_tree: tuple, metadata: dict = None, flat_data: dict = None\n) -&gt; dict:\n    \"\"\"Convert document tree (nested dict) to flat tables data model to prepare database import\n\n    Args:\n        document_tree: A tuple (node_type, content, hash) containing the document tree\n        metadata: A dict of metadata values to add to the root table (a value for each key defined in\n            `metadata_columns` passed to model config)\n        flat_data: A dict to store the flat data into\n\n    Returns:\n        A dict containing flat tables\n    \"\"\"\n\n    def _extract_node(\n        node: tuple, pk_parent_node: int, row_number: int, data_model: dict\n    ) -&gt; int:\n        \"\"\"Extract nodes recursively\n\n        Args:\n            node: A tuple (node_type, content, hash) containing a node of the document tree\n            pk_parent_node: The primary key of its parent node\n            row_number: The row number of the record\n            data_model: The dict to write output to\n\n        Returns:\n            The primary key given to this node\n        \"\"\"\n\n        node_type, content, node_hash = node\n\n        # get the corresponding table model\n        model_table = self.model.tables[node[0]]\n\n        # initialize data structure\n        if node_type not in data_model:\n            data_model[node_type] = {\"next_pk\": 1, \"records\": []}\n            if model_table.is_reused:\n                data_model[node_type][\"hashmap\"] = {}\n            if any(\n                [\n                    rel.other_table.is_reused\n                    for rel in model_table.relations_n.values()\n                ]\n            ):\n                data_model[node_type][\"relations_n\"] = {\n                    rel.rel_table_name: {\"next_pk\": 1, \"records\": []}\n                    for rel in model_table.relations_n.values()\n                    if rel.other_table.is_reused\n                }\n        data = data_model[node_type]\n\n        # if node is reused and a record with identical hash is already inserted, return its pk\n        if model_table.is_reused:\n            if node_hash in data[\"hashmap\"]:\n                return data[\"hashmap\"][node_hash]\n\n        record = {}\n\n        # add pk\n        record_pk = data[\"next_pk\"]\n        record[f\"temp_pk_{model_table.name}\"] = record_pk\n        data[\"next_pk\"] += 1\n\n        # add parent pk if node is not reused\n        if not model_table.is_reused:\n            record[f\"temp_fk_parent_{model_table.parent.name}\"] = pk_parent_node\n            if self.model.model_config[\"row_numbers\"]:\n                record[\"xml2db_row_number\"] = row_number\n\n        # build record from fields for columns and n-1 relations\n        for field_type, key, field in model_table.fields:\n            if field_type == \"col\":\n                content_key = (\n                    (f\"{key[:-5]}__attr\" if field.has_suffix else f\"{key}__attr\")\n                    if field.is_attr\n                    else key\n                )\n                if content_key in content:\n                    val = content[content_key]\n\n                    if len(val) == 1:\n                        record[key] = val[0]\n                    else:\n                        esc_val = [str(v).replace('\"', '\\\\\"') for v in val]\n                        esc_val = [\n                            (\n                                f'\"{v}\"'\n                                if \",\" in v or \"\\n\" in v or \"\\r\" in v or '\"' in v\n                                else v\n                            )\n                            for v in esc_val\n                        ]\n                        record[key] = \",\".join(esc_val)\n                else:\n                    record[key] = None\n\n            elif field_type == \"rel1\":\n                rel = model_table.relations_1[key]\n                if key in content:\n                    record[f\"temp_{rel.field_name}\"] = _extract_node(\n                        content[key][0],\n                        record_pk,\n                        0,\n                        data_model,\n                    )\n                else:\n                    record[f\"temp_{rel.field_name}\"] = None\n\n        # write metadata if it is the root table\n        if pk_parent_node == 0 and isinstance(metadata, dict):\n            for meta_col in self.model.model_config.get(\"metadata_columns\", []):\n                if meta_col[\"name\"] in metadata:\n                    record[meta_col[\"name\"]] = metadata[meta_col[\"name\"]]\n\n        record[self.model.model_config[\"record_hash_column_name\"]] = node_hash\n\n        # add n-n relationship data for reused children nodes\n        for rel in model_table.relations_n.values():\n            if rel.name in content:\n                if rel.other_table.is_reused:\n                    rel_data = data[\"relations_n\"][rel.rel_table_name]\n                    i = 1\n                    for rel_child in content[rel.name]:\n                        rel_row = {\n                            f\"temp_fk_{model_table.name}\": record_pk,\n                            f\"temp_fk_{rel.other_table.name}\": _extract_node(\n                                rel_child,\n                                record_pk,\n                                i,\n                                data_model,\n                            ),\n                        }\n                        if self.model.model_config[\"row_numbers\"]:\n                            rel_row[\"xml2db_row_number\"] = i\n                        rel_data[\"records\"].append(rel_row)\n                        i += 1\n                else:\n                    i = 1\n                    for rel_child in content[rel.name]:\n                        _extract_node(rel_child, record_pk, i, data_model)\n                        i += 1\n\n        data[\"records\"].append(record)\n\n        if model_table.is_reused:\n            data[\"hashmap\"][node_hash] = record_pk\n\n        return record_pk\n\n    flat_tables = flat_data if flat_data else {}\n    _extract_node(document_tree, 0, 0, flat_tables)\n\n    return flat_tables\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.extract_from_database","title":"<code>extract_from_database(root_table_name, root_select_where, force_tz=None)</code>","text":"<p>Extract a subtree from the database and store it in a flat format</p> <p>Parameters:</p> Name Type Description Default <code>root_table_name</code> <code>str</code> <p>The root table name to start from</p> required <code>root_select_where</code> <code>str</code> <p>A where clause to apply to this root table</p> required <code>force_tz</code> <code>Union[str, None]</code> <p>Apply this timezone if database returns timezone-na\u00efve datetime</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A shallow dict of flat data tables</p> Source code in <code>xml2db/document.py</code> <pre><code>def extract_from_database(\n    self,\n    root_table_name: str,\n    root_select_where: str,\n    force_tz: Union[str, None] = None,\n) -&gt; dict:\n    \"\"\"Extract a subtree from the database and store it in a flat format\n\n    Args:\n        root_table_name: The root table name to start from\n        root_select_where: A where clause to apply to this root table\n        force_tz: Apply this timezone if database returns timezone-na\u00efve datetime\n\n    Returns:\n        A shallow dict of flat data tables\n    \"\"\"\n\n    if force_tz:\n        force_tz = ZoneInfo(force_tz)\n\n    def _fetch_data(\n        sqla_table: Table,\n        key_column: Column,\n        join_sequence: list[tuple[Column, Table, Column]],\n        top_where_clause: TextClause,\n        order_by: Union[None, tuple[Column]],\n        append_to: list,\n        conn: Connection,\n    ):\n        \"\"\"Fetch data from a specific table and write fetched rows in a dict keyed by the first row column\"\"\"\n        quer = select(*(sqla_table.columns.values()))\n\n        join_sequence = join_sequence.copy()\n        if len(join_sequence) &gt; 0:\n            left_col, join_tb, right_col = join_sequence.pop()\n            sub_quer = select(right_col)\n            prev_join_col = left_col\n            for left_col, join_tb, right_col in reversed(join_sequence):\n                sub_quer = sub_quer.join(join_tb, right_col == prev_join_col)\n                prev_join_col = left_col\n            sub_quer = sub_quer.where(top_where_clause)\n            quer = quer.where(key_column.in_(sub_quer))\n        else:\n            quer = quer.where(top_where_clause)\n\n        if order_by:\n            quer = quer.order_by(*order_by)\n\n        def add_tz(x):\n            if (\n                force_tz\n                and isinstance(x, datetime.datetime)\n                and (x.tzinfo is None or x.tzinfo.utcoffset(x) is None)\n            ):\n                x = x.replace(tzinfo=force_tz)\n            return x\n\n        col_names = sqla_table.columns.keys()\n        for row in conn.execute(quer):\n            append_to.append({key: add_tz(val) for key, val in zip(col_names, row)})\n\n    def _do_extract_table(\n        tb,\n        top_where_clause,\n        parent_table,\n        join_sequence,\n        res_dict,\n        conn,\n    ):\n        \"\"\"Fetch tables and relationship tables recursively\"\"\"\n        if tb.type_name not in res_dict:\n            res_dict[tb.type_name] = {\"records\": []}\n        _fetch_data(\n            tb.table,\n            (\n                getattr(tb.table.c, f\"pk_{tb.name}\")\n                if tb.is_reused\n                else getattr(tb.table.c, f\"fk_parent_{parent_table.name}\")\n            ),\n            join_sequence,\n            top_where_clause,\n            (\n                None\n                if tb.is_reused or not tb.data_model.model_config[\"row_numbers\"]\n                else (\n                    getattr(tb.table.c, f\"fk_parent_{parent_table.name}\"),\n                    tb.table.c.xml2db_row_number,\n                )\n            ),\n            res_dict[tb.type_name][\"records\"],\n            conn,\n        )\n        join_root = (\n            [(None, tb.table, getattr(tb.table.c, f\"pk_{tb.name}\"))]\n            if parent_table is None\n            else []\n        )\n        if len(tb.relations_n) &gt; 0:\n            if \"relations_n\" not in res_dict[tb.type_name]:\n                res_dict[tb.type_name][\"relations_n\"] = {}\n            for rel in tb.relations_n.values():\n                if rel.rel_table_name not in res_dict[tb.type_name][\"relations_n\"]:\n                    res_dict[tb.type_name][\"relations_n\"][rel.rel_table_name] = {\n                        \"records\": []\n                    }\n                new_join = []\n                if not tb.is_reused:\n                    new_join = [\n                        (\n                            getattr(tb.table.c, f\"fk_parent_{parent_table.name}\"),\n                            tb.table,\n                            getattr(tb.table.c, f\"pk_{tb.table.name}\"),\n                        )\n                    ]\n                if rel.other_table.is_reused:\n                    _fetch_data(\n                        rel.rel_table,\n                        getattr(rel.rel_table.c, f\"fk_{tb.name}\"),\n                        join_sequence + join_root + new_join,\n                        top_where_clause,\n                        (\n                            (\n                                getattr(rel.rel_table.c, f\"fk_{tb.name}\"),\n                                rel.rel_table.c.xml2db_row_number,\n                            )\n                            if tb.data_model.model_config[\"row_numbers\"]\n                            else None\n                        ),\n                        res_dict[tb.type_name][\"relations_n\"][rel.rel_table_name][\n                            \"records\"\n                        ],\n                        conn,\n                    )\n                    new_join = new_join + [\n                        (\n                            getattr(rel.rel_table.c, f\"fk_{tb.name}\"),\n                            rel.rel_table,\n                            getattr(rel.rel_table.c, f\"fk_{rel.other_table.name}\"),\n                        )\n                    ]\n                _do_extract_table(\n                    rel.other_table,\n                    top_where_clause,\n                    tb,\n                    join_sequence + join_root + new_join,\n                    res_dict,\n                    conn,\n                )\n        for rel in tb.relations_1.values():\n            _do_extract_table(\n                rel.other_table,\n                top_where_clause,\n                tb,\n                join_sequence\n                + [\n                    (\n                        getattr(\n                            tb.table.c,\n                            (\n                                f\"pk_{tb.name}\"\n                                if tb.is_reused\n                                else f\"fk_parent_{parent_table.name}\"\n                            ),\n                        ),\n                        tb.table,\n                        getattr(tb.table.c, f\"{rel.field_name}\"),\n                    )\n                ],\n                res_dict,\n                conn,\n            )\n\n    flat_tables = {}\n\n    with self.model.engine.connect() as conn:\n        _do_extract_table(\n            self.model.tables[root_table_name],\n            text(root_select_where),\n            None,\n            [],\n            flat_tables,\n            conn,\n        )\n\n    self.data = flat_tables\n    return flat_tables\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.flat_data_to_doc_tree","title":"<code>flat_data_to_doc_tree()</code>","text":"<p>Convert the data stored in flat tables into a document tree</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple (node_type, content, hash) containing the document tree</p> Source code in <code>xml2db/document.py</code> <pre><code>def flat_data_to_doc_tree(self) -&gt; tuple:\n    \"\"\"Convert the data stored in flat tables into a document tree\n\n    Returns:\n        A tuple (node_type, content, hash) containing the document tree\n    \"\"\"\n    data_index = {}\n\n    # convert data to keyed dict for easier access\n    temp = (\n        \"\"\n        if f\"pk_{self.model.tables[self.model.root_table].name}\"\n        in self.data[self.model.root_table][\"records\"][0]\n        else \"temp_\"\n    )\n    for tb in self.model.tables.values():\n        data_index[tb.type_name] = {\n            \"records\": {},\n            \"relations_n\": {},\n        }\n        if tb.type_name in self.data:\n            data_index[tb.type_name][\"records\"] = {\n                row[f\"{temp}pk_{tb.name}\"]: row\n                for row in self.data[tb.type_name][\"records\"]\n            }\n        for rel in tb.relations_n.values():\n            index = {}\n            if rel.other_table.is_reused:\n                if tb.type_name in self.data:\n                    for row in self.data[tb.type_name][\"relations_n\"][\n                        rel.rel_table_name\n                    ][\"records\"]:\n                        if row[f\"{temp}fk_{tb.name}\"] not in index:\n                            index[row[f\"{temp}fk_{tb.name}\"]] = []\n                        index[row[f\"{temp}fk_{tb.name}\"]].append(\n                            row[f\"{temp}fk_{rel.other_table.name}\"]\n                        )\n            else:\n                if rel.other_table.type_name in self.data:\n                    for row in self.data[rel.other_table.type_name][\"records\"]:\n                        if row[f\"{temp}fk_parent_{tb.name}\"] not in index:\n                            index[row[f\"{temp}fk_parent_{tb.name}\"]] = []\n                        index[row[f\"{temp}fk_parent_{tb.name}\"]].append(\n                            row[f\"{temp}pk_{rel.other_table.name}\"]\n                        )\n            data_index[tb.type_name][\"relations_n\"][rel.rel_table_name] = index\n\n    def _build_node(node_type: str, node_pk: int) -&gt; tuple:\n        \"\"\"Build a dict node recursively\n\n        Args:\n            node_type: The node type\n            node_pk: The node primary key\n\n        Returns:\n            A node as a tuple (node_type, content, hash)\n        \"\"\"\n        tb = self.model.tables[node_type]\n        content = {}\n\n        record = data_index[node_type][\"records\"][node_pk]\n        for field_type, rel_name, rel in tb.fields:\n            if field_type == \"col\" and record[rel_name] is not None:\n                content_key = (\n                    (\n                        f\"{rel_name[:-5]}__attr\"\n                        if rel.has_suffix\n                        else f\"{rel_name}__attr\"\n                    )\n                    if rel.is_attr\n                    else rel_name\n                )\n                if isinstance(record[rel_name], datetime.datetime):\n                    content[content_key] = [\n                        record[rel_name].isoformat(timespec=\"milliseconds\")\n                    ]\n                else:\n                    content[content_key] = (\n                        list(csv.reader([str(record[rel_name])], escapechar=\"\\\\\"))[\n                            0\n                        ]\n                        if \",\" in str(record[rel_name])\n                        else [record[rel_name]]\n                    )\n            elif (\n                field_type == \"rel1\"\n                and record[f\"{temp}{rel.field_name}\"] is not None\n            ):\n                content[rel_name] = [\n                    _build_node(\n                        rel.other_table.type_name, record[f\"{temp}{rel.field_name}\"]\n                    )\n                ]\n            elif (\n                field_type == \"reln\"\n                and node_pk\n                in data_index[tb.type_name][\"relations_n\"][rel.rel_table_name]\n            ):\n                content[rel_name] = [\n                    _build_node(rel.other_table.type_name, pk)\n                    for pk in data_index[tb.type_name][\"relations_n\"][\n                        rel.rel_table_name\n                    ][node_pk]\n                ]\n        return node_type, content\n\n    return _build_node(\n        self.model.root_table,\n        int(list(data_index[self.model.root_table][\"records\"].keys())[0]),\n    )\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.insert_into_target_tables","title":"<code>insert_into_target_tables(single_transaction=True, max_lines=-1)</code>","text":"<p>Insert and merge data into the database</p> <p>Insert data into temporary tables and then merge temporary tables into target tables.</p> <p>Parameters:</p> Name Type Description Default <code>single_transaction</code> <code>bool</code> <p>Should we run all queries in a single transaction, or isolate queries at the minimum scope required to ensure database consistency?</p> <code>True</code> <code>max_lines</code> <code>int</code> <p>The maximum number of lines to insert in a single statement when loading data to the temporary tables</p> <code>-1</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of inserted rows</p> Source code in <code>xml2db/document.py</code> <pre><code>def insert_into_target_tables(\n    self,\n    single_transaction: bool = True,\n    max_lines: int = -1,\n) -&gt; int:\n    \"\"\"Insert and merge data into the database\n\n    Insert data into temporary tables and then merge temporary tables into target tables.\n\n    Args:\n        single_transaction: Should we run all queries in a single transaction, or isolate queries at the minimum\n            scope required to ensure database consistency?\n        max_lines: The maximum number of lines to insert in a single statement when loading data to the temporary\n            tables\n\n    Returns:\n        The number of inserted rows\n    \"\"\"\n    try:\n        self.model.create_db_schema()\n    except Exception as e:\n        logger.error(\n            f\"Error while creating database schema '{self.model.db_schema}'\"\n        )\n        logger.error(e)\n        raise\n    try:\n        self.insert_into_temp_tables(max_lines)\n    except Exception as e:\n        logger.error(\n            f\"Error while importing into temporary tables from {self.xml_file_path}\"\n        )\n        logger.error(e)\n        raise\n    else:\n        logger.info(\n            f\"Merging temporary tables into target tables for {self.xml_file_path}\"\n        )\n        try:\n            self.model.create_all_tables()  # Create target tables if not exist\n            inserted_rows = self.merge_into_target_tables(single_transaction)\n        except Exception as e:\n            logger.error(\n                f\"Error while merging temporary tables into target tables for {self.xml_file_path}\"\n            )\n            logger.error(e)\n            raise\n    finally:\n        logger.info(f\"Dropping temporary tables for {self.xml_file_path}\")\n        self.model.drop_all_temp_tables()\n\n    return inserted_rows\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.insert_into_temp_tables","title":"<code>insert_into_temp_tables(max_lines=-1)</code>","text":"<p>Insert data into temporary tables</p> <p>(Re)creates temp tables before inserting data.</p> <p>Parameters:</p> Name Type Description Default <code>max_lines</code> <code>int</code> <p>The maximum number of lines to insert in a single statement</p> <code>-1</code> Source code in <code>xml2db/document.py</code> <pre><code>def insert_into_temp_tables(self, max_lines: int = -1) -&gt; None:\n    \"\"\"Insert data into temporary tables\n\n    (Re)creates temp tables before inserting data.\n\n    Args:\n        max_lines: The maximum number of lines to insert in a single statement\n    \"\"\"\n    logger.info(f\"Dropping temp tables if exist for {self.xml_file_path}\")\n    self.model.drop_all_temp_tables()\n\n    logger.info(f\"Creating temp tables for {self.xml_file_path}\")\n    self.model.create_all_tables(temp=True)\n\n    logger.info(f\"Inserting data into temporary tables from {self.xml_file_path}\")\n    # insert data (order does not really matter)\n    for tb in self.model.fk_ordered_tables:\n        for query, data in tb.get_insert_temp_records_statements(\n            self.data.get(tb.type_name, None)\n        ):\n            if max_lines is None or max_lines &lt; 0:\n                max_lines = len(data)\n            start_idx = 0\n            while start_idx &lt; len(data):\n                with self.model.engine.begin() as conn:\n                    conn.execute(query, data[start_idx : (start_idx + max_lines)])\n                start_idx = start_idx + max_lines\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.merge_into_target_tables","title":"<code>merge_into_target_tables(single_transaction=True)</code>","text":"<p>Merge data into target data model</p> <p>Execute all update and insert statements needed to merge temporary tables content into target tables.</p> <p>Parameters:</p> Name Type Description Default <code>single_transaction</code> <code>bool</code> <p>Should we run all queries in a single transaction, or isolate queries at the minimum scope required to ensure database consistency?</p> <code>True</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of inserted rows</p> Source code in <code>xml2db/document.py</code> <pre><code>def merge_into_target_tables(self, single_transaction: bool = True) -&gt; int:\n    \"\"\"Merge data into target data model\n\n    Execute all update and insert statements needed to merge temporary tables content into target tables.\n\n    Args:\n        single_transaction: Should we run all queries in a single transaction, or isolate queries at the minimum\n            scope required to ensure database consistency?\n\n    Returns:\n        The number of inserted rows\n    \"\"\"\n    inserted_rows_count = 0\n    for tables in (\n        [self.model.fk_ordered_tables]\n        if single_transaction\n        else self.model.transaction_groups\n    ):\n        with self.model.engine.begin() as conn:\n            for tb in tables:\n                for query in tb.get_merge_temp_records_statements():\n                    result = conn.execute(query)\n                    if query.is_insert:\n                        inserted_rows_count += result.rowcount\n    if inserted_rows_count == 0:\n        logger.info(\"No rows were inserted!\")\n    else:\n        logger.info(f\"Inserted rows: {inserted_rows_count}\")\n\n    return inserted_rows_count\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.parse_xml","title":"<code>parse_xml(xml_file, metadata=None, skip_validation=True, iterparse=True, recover=False, flat_data=None)</code>","text":"<p>Parse an XML document and apply transformation corresponding to the target data model</p> <p>This method will first parse the XML file into a dict (document tree) using lxml and then compute hash for all nodes based on their content, and finally convert the document tree to tables data, creating primary keys and relations, ready to be inserted in the database.</p> <p>Parameters:</p> Name Type Description Default <code>xml_file</code> <code>Union[str, BytesIO]</code> <p>The path or the file object of an XML file to parse</p> required <code>metadata</code> <code>dict</code> <p>A dict of metadata values to add to the root table (a value for each key defined in <code>metadata_columns</code> passed to model config)</p> <code>None</code> <code>skip_validation</code> <code>bool</code> <p>Should we validate the document against the schema first?</p> <code>True</code> <code>iterparse</code> <code>bool</code> <p>Parse XML using iterative parsing, which is a bit slower but uses less memory</p> <code>True</code> <code>recover</code> <code>bool</code> <p>Should we try to parse incorrect XML? (argument passed to lxml parser)</p> <code>False</code> <code>flat_data</code> <code>dict</code> <p>A dict containing flat data if we want to add data to another dataset instead of creating a new one</p> <code>None</code> Source code in <code>xml2db/document.py</code> <pre><code>def parse_xml(\n    self,\n    xml_file: Union[str, BytesIO],\n    metadata: dict = None,\n    skip_validation: bool = True,\n    iterparse: bool = True,\n    recover: bool = False,\n    flat_data: dict = None,\n) -&gt; None:\n    \"\"\"Parse an XML document and apply transformation corresponding to the target data model\n\n    This method will first parse the XML file into a dict (document tree) using lxml\n    and then compute hash for all nodes based on their content, and finally convert\n    the document tree to tables data, creating primary keys and relations, ready to\n    be inserted in the database.\n\n    Args:\n        xml_file: The path or the file object of an XML file to parse\n        metadata: A dict of metadata values to add to the root table (a value for each key defined in\n            `metadata_columns` passed to model config)\n        skip_validation: Should we validate the document against the schema first?\n        iterparse: Parse XML using iterative parsing, which is a bit slower but uses less memory\n        recover: Should we try to parse incorrect XML? (argument passed to lxml parser)\n        flat_data: A dict containing flat data if we want to add data to another dataset instead of creating\n            a new one\n    \"\"\"\n    self.xml_file_path = xml_file[:255] if isinstance(xml_file, str) else \"&lt;stream&gt;\"\n\n    document_tree = self.model.xml_converter.parse_xml(\n        xml_file=xml_file,\n        file_path=self.xml_file_path,\n        skip_validation=skip_validation,\n        recover=recover,\n        iterparse=iterparse,\n    )\n\n    if self.model.model_config[\"document_tree_hook\"] is not None:\n        logger.info(f\"Running document_tree_hook function for {self.xml_file_path}\")\n        document_tree = self.model.model_config[\"document_tree_hook\"](document_tree)\n\n    logger.info(f\"Adding records to data model for {self.xml_file_path}\")\n    self.data = self.doc_tree_to_flat_data(\n        document_tree,\n        metadata=metadata,\n        flat_data=flat_data,\n    )\n\n    logger.debug(self.__repr__())\n</code></pre>"},{"location":"api/document/#xml2db.document.Document.to_xml","title":"<code>to_xml(out_file=None, nsmap=None, indent='  ')</code>","text":"<p>Convert a document tree (nested dict) into an XML file</p> <p>Parameters:</p> Name Type Description Default <code>out_file</code> <code>str</code> <p>If provided, write output to a file.</p> <code>None</code> <code>nsmap</code> <code>dict</code> <p>An optional namespace mapping.</p> <code>None</code> <code>indent</code> <code>str</code> <p>A string used as indent in XML output.</p> <code>'  '</code> <p>Returns:</p> Type Description <code>Element</code> <p>The etree object corresponding to the root XML node.</p> Source code in <code>xml2db/document.py</code> <pre><code>def to_xml(\n    self, out_file: str = None, nsmap: dict = None, indent: str = \"  \"\n) -&gt; etree.Element:\n    \"\"\"Convert a document tree (nested dict) into an XML file\n\n    Args:\n        out_file: If provided, write output to a file.\n        nsmap: An optional namespace mapping.\n        indent: A string used as indent in XML output.\n\n    Returns:\n        The etree object corresponding to the root XML node.\n    \"\"\"\n    converter = XMLConverter(self.model)\n    converter.document_tree = self.flat_data_to_doc_tree()\n    return converter.to_xml(out_file=out_file, nsmap=nsmap, indent=indent)\n</code></pre>"},{"location":"api/overview/","title":"API Overview","text":""},{"location":"api/overview/#building-a-data-model-from-an-xsd-file","title":"Building a data model from an XSD file","text":"<ul> <li><code>DataModel</code>: use directly the constructor to create an instance of a data model. </li> </ul> <p>Note</p> <p>You should always use the <code>DataModel</code> constructor to create a new instance instead of trying to change an  instance's attributes, as internal objects are created when the constructor is called.</p>"},{"location":"api/overview/#inspecting-the-data-model","title":"Inspecting the data model","text":"<ul> <li><code>DataModel.source_tree</code>: see the data model in tree format before any transformation</li> <li><code>DataModel.target_tree</code>: see the data model in tree format after simplification (corrsponding to the data model     which will be created in the database)</li> <li><code>DataModel.get_entity_rel_diagram</code>: get a visual      representation of the data model using Mermaid</li> <li><code>DataModel.get_all_create_table_statements</code>:     get SQLAlchemy <code>CREATE TABLE</code> statements that can be printed for detailed inspection</li> </ul>"},{"location":"api/overview/#loading-data-into-the-database","title":"Loading data into the database","text":"<ul> <li><code>DataModel.parse_xml</code>: read and parse a XML document, which is     loaded in memory</li> <li><code>Document.insert_into_target_tables</code>: load a file     into the database</li> </ul>"},{"location":"api/overview/#advanced-use-loading-data-into-the-database","title":"Advanced use: loading data into the database","text":"<p>The flow chart below presents data conversions used to load an XML file into the database, showing the functions used  for lower level steps. It can be useful for advanced use cases, for instance:</p> <ul> <li>transforming the data in intermediate steps,</li> <li>adding logging,</li> <li>limiting concurrent access to the database within a multiprocess setup, etc.</li> </ul> <p>For those scenarios you can easily reimplement  <code>Document.insert_into_target_tables</code> to suit your  needs, using lower level functions.</p> <pre><code>flowchart TB\n    subgraph \"&lt;a href='../data_model/#xml2db.model.DataModel.parse_xml' style='color:var(--md-code-fg-color)'&gt;DataModel.parse_xml&lt;/a&gt;\"\n        direction TB\n        A[XML file]-- \"&lt;a href='../xml_converter/#xml2db.xml_converter.XMLConverter.parse_xml' style='color:var(--md-code-fg-color)'&gt;XMLConverter.parse_xml&lt;/a&gt;\" --&gt;B[Document tree]\n        B-- \"&lt;a href='../document/#xml2db.document.Document.doc_tree_to_flat_data' style='color:var(--md-code-fg-color)'&gt;Document.doc_tree_to_flat_data&lt;/a&gt;\" --&gt;C[Flat data model]\n    end\n    C -.- D\n    subgraph \"&lt;a href='../document/#xml2db.document.Document.insert_into_target_tables' style='color:var(--md-code-fg-color)'&gt;Document.insert_into_target_tables&lt;/a&gt;\"\n        direction TB\n        D[Flat data model]-- \"&lt;a href='../document/#xml2db.document.Document.insert_into_temp_tables' style='color:var(--md-code-fg-color)'&gt;Document.insert_into_temp_tables&lt;/a&gt;\" --&gt;E[Temporary tables]\n        E-- \"&lt;a href='../document/#xml2db.document.Document.merge_into_target_tables' style='color:var(--md-code-fg-color)'&gt;Document.merge_into_target_tables&lt;/a&gt;\" --&gt;F[Target tables]\n    end</code></pre>"},{"location":"api/overview/#advanced-use-get-data-from-the-database-back-to-xml","title":"Advanced use: get data from the database back to XML","text":"<p>The flow chart below presents data conversions used to get back data from the database into XML, showing the functions  used for lower level steps.</p> <pre><code>flowchart TB\n    subgraph \"&lt;a href='../data_model/#xml2db.model.DataModel.extract_from_database' style='color:var(--md-code-fg-color)'&gt;DataModel.extract_from_database&lt;/a&gt;\"\n        direction TB\n        A[Database]--&gt;B[Flat data model]    \n    end\n    B -.- C\n    subgraph \"&lt;a href='../document/#xml2db.document.Document.to_xml' style='color:var(--md-code-fg-color)'&gt;Document.to_xml&lt;/a&gt;\" \n        direction TB\n        C[Flat data model]-- \"&lt;a href='../document/#xml2db.document.Document.flat_data_to_doc_tree' style='color:var(--md-code-fg-color)'&gt;Document.flat_data_to_doc_tree&lt;/a&gt;\" --&gt;D[Document tree]\n        D-- \"&lt;a href='../xml_converter/#xml2db.xml_converter.XMLConverter.to_xml' style='color:var(--md-code-fg-color)'&gt;XMLConverter.to_xml&lt;/a&gt;\" --&gt;E[XML file]\n    end</code></pre>"},{"location":"api/xml_converter/","title":"XMLConverter","text":"<p>A class to convert data from document tree format (nested dict) to and from XML.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>DataModel</code> <p>The <code>DataModel</code> object used to parse XML files</p> required <code>document_tree</code> <code>dict</code> <p>Data in the document tree format (optional, can be built later by the <code>parse_xml</code> method)</p> <code>None</code> Source code in <code>xml2db/xml_converter.py</code> <pre><code>def __init__(self, data_model: \"DataModel\", document_tree: dict = None):\n    \"\"\"A class to convert data from document tree format (nested dict) to and from XML.\n\n    Args:\n        data_model: The [`DataModel`](./data_model.md#xml2db.model.DataModel) object used to parse XML files\n        document_tree: Data in the document tree format (optional, can be built later by the `parse_xml` method)\n    \"\"\"\n    self.model = data_model\n    self.document_tree = document_tree\n</code></pre>"},{"location":"api/xml_converter/#xml2db.xml_converter.XMLConverter.parse_xml","title":"<code>parse_xml(xml_file, file_path=None, skip_validation=False, recover=False, iterparse=True)</code>","text":"<p>Parse an XML document into a nested dict and performs the simplifications defined in the DataModel object (\"pull\" child to upper level, transform a choice model into \"type\" and \"value\" fields or concatenate children as string).</p> <p>Parameters:</p> Name Type Description Default <code>xml_file</code> <code>Union[str, BytesIO]</code> <p>An XML file path or file content to be converted</p> required <code>file_path</code> <code>str</code> <p>The file path to be printed in logs</p> <code>None</code> <code>skip_validation</code> <code>bool</code> <p>Whether we should validate XML against the schema before parsing</p> <code>False</code> <code>recover</code> <code>bool</code> <p>Try to process malformed XML (lxml option)</p> <code>False</code> <code>iterparse</code> <code>bool</code> <p>Parse XML using iterative parsing, which is a bit slower but uses less memory</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple</code> <p>The parsed data in the document tree format (nested dict)</p> Source code in <code>xml2db/xml_converter.py</code> <pre><code>def parse_xml(\n    self,\n    xml_file: Union[str, BytesIO],\n    file_path: str = None,\n    skip_validation: bool = False,\n    recover: bool = False,\n    iterparse: bool = True,\n) -&gt; tuple:\n    \"\"\"Parse an XML document into a nested dict and performs the simplifications defined in the\n    DataModel object (\"pull\" child to upper level, transform a choice model into \"type\" and \"value\"\n    fields or concatenate children as string).\n\n    Args:\n        xml_file: An XML file path or file content to be converted\n        file_path: The file path to be printed in logs\n        skip_validation: Whether we should validate XML against the schema before parsing\n        recover: Try to process malformed XML (lxml option)\n        iterparse: Parse XML using iterative parsing, which is a bit slower but uses less memory\n\n    Returns:\n        The parsed data in the document tree format (nested dict)\n    \"\"\"\n\n    xt = None\n    if not iterparse or (not skip_validation and recover):\n        logger.info(\"Parsing XML file\")\n        xt = etree.parse(xml_file, parser=etree.XMLParser(recover=recover))\n\n    if skip_validation:\n        logger.info(\"Skipping XML file validation\")\n    else:\n        logger.info(\"Validating XML file against the schema\")\n        if not self.model.lxml_schema.validate(xt if xt else etree.parse(xml_file)):\n            logger.error(f\"XML file {file_path} does not conform with the schema\")\n            raise ValueError(\n                f\"XML file {file_path} does not conform with the schema\"\n            )\n        logger.info(\"XML file conforms with the schema\")\n\n    if iterparse:\n        self.document_tree = self._parse_iterative(xml_file, recover)\n    else:\n        self.document_tree = self._parse_element_tree(xt)\n\n    return self.document_tree\n</code></pre>"},{"location":"api/xml_converter/#xml2db.xml_converter.XMLConverter.to_xml","title":"<code>to_xml(out_file=None, nsmap=None, indent='  ')</code>","text":"<p>Convert a document tree (nested dict) into an XML file</p> <p>Parameters:</p> Name Type Description Default <code>out_file</code> <code>str</code> <p>If provided, write output to a file.</p> <code>None</code> <code>nsmap</code> <code>dict</code> <p>An optional namespace mapping.</p> <code>None</code> <code>indent</code> <code>str</code> <p>A string used as indentin XML output.</p> <code>'  '</code> <p>Returns:</p> Type Description <code>Element</code> <p>The etree object corresponding to the root XML node.</p> Source code in <code>xml2db/xml_converter.py</code> <pre><code>def to_xml(\n    self, out_file: str = None, nsmap: dict = None, indent: str = \"  \"\n) -&gt; etree.Element:\n    \"\"\"Convert a document tree (nested dict) into an XML file\n\n    Args:\n        out_file: If provided, write output to a file.\n        nsmap: An optional namespace mapping.\n        indent: A string used as indentin XML output.\n\n    Returns:\n        The etree object corresponding to the root XML node.\n    \"\"\"\n    doc = self._make_xml_node(\n        self.document_tree,\n        self.model.tables[self.document_tree[0]].name,\n        nsmap,\n    )\n    if self.model.tables[self.model.root_table].is_virtual_node:\n        child = None\n        for child in doc:\n            break\n        doc = child\n    if out_file:\n        etree.indent(doc, space=indent)\n        with open(out_file, \"wt\") as f:\n            f.write(\n                etree.tostring(\n                    doc,\n                    pretty_print=True,\n                    encoding=\"utf-8\",\n                    xml_declaration=True,\n                ).decode(\"utf-8\")\n            )\n    return doc\n</code></pre>"}]}